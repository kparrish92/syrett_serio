---
title: "Experiment 2"
output: html_document
date: "2024-10-31"
---

# Overview 

This page contains the analysis for experiment 2 (the three alternative forced choice task). As we discussed, I've included the code and comments in this document so that the analysis pipeline is viewable. Within this page, I've included several section including a statistical analysis section, a section about prior selection, a model reporting section, pairwise comparisons, and, finally, a sensitivity analysis (for the appendix). 


The following code loads the needed libraries and the data from experiment 1. This data was used to create priors. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
# This script uses the raw data from exp 1 to generate priors for experiment 2.
library(here)
library(tidyverse)
library(brms)
library(bayestestR)


# Note: the files need to be inside a folder called "tidy", which is inside a folder called "data". If it's somewhere else, you can adjust these lines to read the data.
nonmember_tidy = read.csv(here("data", "tidy", "nonmember_tidy.csv"))
member_tidy = read.csv(here("data", "tidy", "member_tidy.csv"))

```

This code takes both the member and nonmember prompts and returns the total number of trials in the data set for each condition, to be used later in the determination of proprotions of each predicted repsonse based on the ratings.

```{r}
# make a grouped dataframe to identify the total number of trials per main condition (denominator of probabiliy)
total_possible_nm_df = nonmember_tidy %>% 
  group_by(CategoryType) %>% 
  summarise(n = n())

total_possible_m_df = member_tidy %>% 
  group_by(CategoryType) %>% 
  summarise(n = n())

# assign them to an object 
nm_total = total_possible_nm_df$n[1] 
m_total = total_possible_m_df$n[1] 
```

This is how the priors for experiment 2 were created based on the data from experiment 1. The rating data was conditionally mutated depending upon whether it came from the `nonmember_tidy` or `member_tidy` data set. For the `nonmember` data, ratings of 1 or 2 were considered to be "is" answers, 3, 4 and 5 were "both" answers, and 6 and 7 were "is not" answers. On the other hand, the `member_tidy` predicted answers were essentially the opposite: 1 and 2 were predicted to be "is not" answers, 3, 4, 5 and were "both" answers, and 6 and 7 were "is" answers.  

```{r}
# Mutate the original data conditionally 
## For nonmember, 1 and 2 are ratings are predicted to be "is" answers.
## 3,4,and 5 are predicted to be "both" answers.
## 6 and 7 are predicted to be "is not" answers.
nonmember_tidy_mutated = nonmember_tidy %>% 
  mutate(predicted_answer = case_when(
    Rating == 1 | Rating == 2 ~ "is",
    Rating == 3 | Rating == 4 | Rating == 5 ~ "both",
    Rating == 6 | Rating == 7 ~ "is_not",
  ))
## For member, it is revered from nonmember:
## 1 and 2 are ratings are predicted to be "is not" answers.
## 3,4,and 5 are predicted to be "both" answers.
## 6 and 7 are predicted to be "is" answers.  
member_tidy_mutated = member_tidy %>% 
  mutate(predicted_answer = case_when(
    Rating == 1 | Rating == 2 ~ "is_not",
    Rating == 3 | Rating == 4 | Rating == 5 ~ "both",
    Rating == 6 | Rating == 7 ~ "is",
  ))
```

Next, the data was grouped by the `CategoryType` and the `predicted_answer` (described above) to return the total number of answers for each type in the data for both nonmember and member data. 

```{r}
# calculate the proportion of predicted is_not, both, and is answers for both member and non member data.
nonmember_tidy_mutated_grouped = nonmember_tidy_mutated %>% 
  group_by(CategoryType, predicted_answer) %>% 
  summarise(n_a = n())

member_tidy_mutated_grouped = member_tidy_mutated %>% 
  group_by(CategoryType, predicted_answer) %>% 
  summarise(n_b = n())

nonmember_tidy_mutated_grouped
member_tidy_mutated_grouped
```

Now, using the grouped data, a probability was generated by dividing the grouped totals by the total number possible in that category.

```{r}
# Join the member and nonmember data and calculate the proprotion of predicted responses relative to the total number of trials - our priors in probability.
data_for_priors = left_join(nonmember_tidy_mutated_grouped, member_tidy_mutated_grouped) %>% 
  mutate(predicted_grp_prob = (n_a + n_b)/sum(m_total, nm_total))

```

The table below shows the probability of each of the 9 posible combinations of answer and condition. 

```{r}
data_for_priors %>% 
  filter(CategoryType == "Abstract" | CategoryType == "Artifact" | CategoryType == "Natural") %>% 
  select(CategoryType, predicted_answer, predicted_grp_prob) %>% 
  pivot_wider(names_from = predicted_answer, values_from = predicted_grp_prob) %>% 
  knitr::kable(format = "pandoc", digits = 2)
```

To incorporate this information in the model, it was necessary to do some arithmetic. The model has two intercepts that are specified directly, and a third that is not. In addition to that, there are four fixed effects that we also can specify. The intercepts represent the probability of the choosing a given answer for the reference level, while the effects represent the change in the log-odds of the same answer going from one `CategoryType` to another. So, in our model, we have an intercept for "is" and "is not" that we specify, and the remaining probability space is probability of "both" for the reference level (`Value`). The effects specify both an intercept and an effect and are the difference in log-odds relative to the log-odds for that answer of `Value`. 

To report this process, I would simply say something like this: In order specify our prior beliefs about the probabilities for each answer and category type, probabilities were converted to log-odds and calculated relative to a given reference level.

This process is one I don't think is super necessary to get down in the weeds on (for the purpose of a paper or reporting), but I am happy to elaborate further if this is not clear/you are curious. 

```{r}
sd = .5
is_int = paste0("normal(", qlogis(.29), ", ",sd,")")
intnot_int = paste0("normal(", qlogis(.36), ", ",sd,")")
is_art = paste0("normal(", 1.3, ", ",sd,")")
is_nat = paste0("normal(", 1.7, ", ",sd,")")
is_not_art = paste0("normal(", .81, ", ",sd,")")
is_not_nat = paste0("normal(", 1.38, ", ",sd,")")

prior_check = c(set_prior(is_int, class = "Intercept", dpar = "muIs"),
                set_prior(intnot_int, class = "Intercept", dpar = "muIsnot"),
                set_prior(is_art, class = "b", dpar = "muIs", coef = "CategoryTypeArtifact"),
                set_prior(is_nat, class = "b", dpar = "muIs", coef = "CategoryTypeNatural"),
                set_prior(is_not_art, class = "b", dpar = "muIsnot", coef = "CategoryTypeArtifact"),
                set_prior(is_not_nat, class = "b", dpar = "muIsnot", coef = "CategoryTypeNatural"))


```

Now, the data is loaded in for experiment 2 and the reference was set to `Value`. Next the model is fit. If your machine does not `brms` in R, this will probably not run. When it does run, this model takes a few minutes to fit (around 5).


```{r}
## Should fit this in a script and laod in in the rmd for the sake of time 
senses_tidy = read.csv(here("data", "tidy", "senses_tidy.csv")) 

senses_tidy$CategoryType = as.factor(senses_tidy$CategoryType)
senses_tidy$CategoryType = relevel(senses_tidy$CategoryType, ref = "Value")

b2 <- brm(Selection ~ CategoryType + (CategoryType | Participant), 
          prior = prior_check,
          data=senses_tidy,
          family="categorical")
```

The reference level, for our purposes, is arbitrary, since we will make conclusions on the basis of the pairwise comparisons. In other words, the reference level would change the model terms, but when we do the math to figure out the probability of each condition it would stay the same. In other words, the plot directly below this would not change, but the model summary would.


```{r}
conditional_effects(b2, categorical = TRUE)
```

```{r}
describe_posterior(b2) %>% 
  as_tibble() %>% 
  select(-c("CI", "ROPE_CI", "ROPE_low", "ROPE_high")) %>% 
  mutate(Parameter = case_when(
    Parameter == "b_muIs_Intercept" ~ "Intercept 'is'", 
    Parameter == "b_muIsnot_Intercept" ~ "Intercept 'is not'", 
    Parameter == "b_muIs_CategoryTypeArtifact" ~ "Is_CategoryTypeArtifact", 
    Parameter == "b_muIs_CategoryTypeNatural" ~ "Is_CategoryTypeNatural", 
    Parameter == "b_muIsnot_CategoryTypeArtifact" ~ "Isnot_CategoryTypeArtifact",
    Parameter == "b_muIsnot_CategoryTypeNatural" ~ "Isnot_CategoryTypeNatural"
  )) %>% 
  mutate(ESS = round(ESS)) %>% 
  knitr::kable(format = "pandoc", digits = 2)

```

```{r}
b2_default <- brm(Selection ~ CategoryType + (CategoryType | Participant), 
          data=senses_tidy,
          family="categorical")

conditional_effects(b2_default, categorical = TRUE)
```


```{r}
describe_posterior(b2_default) %>% 
  as_tibble() %>% 
  select(-c("CI", "ROPE_CI", "ROPE_low", "ROPE_high")) %>% 
  mutate(Parameter = case_when(
    Parameter == "b_muIs_Intercept" ~ "Intercept 'is'", 
    Parameter == "b_muIsnot_Intercept" ~ "Intercept 'is not'", 
    Parameter == "b_muIs_CategoryTypeArtifact" ~ "Is_CategoryTypeArtifact", 
    Parameter == "b_muIs_CategoryTypeNatural" ~ "Is_CategoryTypeNatural", 
    Parameter == "b_muIsnot_CategoryTypeArtifact" ~ "Isnot_CategoryTypeArtifact",
    Parameter == "b_muIsnot_CategoryTypeNatural" ~ "Isnot_CategoryTypeNatural"
  )) %>% 
  mutate(ESS = round(ESS))  %>% 
  knitr::kable(format = "pandoc", digits = 2)
```